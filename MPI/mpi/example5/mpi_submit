#!/bin/bash

#SBATCH --nodes 1
#SBATCH --ntasks-per-node 4
#SBATCH --partition veryshort
#SBATCH --account COMS30005
#SBATCH --job-name MPI
#SBATCH --time 00:15:00
#SBATCH --output OUT
#SBATCH --exclusive

# This time, asking for 1 node with 4 tasks per node

# Use Intel MPI (make sure you compile with the same module and 'mpiicc')
module load languages/intel/2018-u3


# Print some information about the job
echo "Running on host $(hostname)"
echo "Time is $(date)"
echo "Directory is $(pwd)"
echo "Slurm job ID is $SLURM_JOB_ID"
echo
echo "This job runs on the following machines:"
echo "$SLURM_JOB_NODELIST" | uniq
echo


# Enable using `srun` with Intel MPI
export I_MPI_PMI_LIBRARY=/usr/lib64/libpmi.so

# Run the parallel MPI executable
# Just run the first binary: change this line to try out the others
echo "skeleton 1"
echo
srun ./skeleton1.exe

echo
echo "skeleton 2"
echo
srun ./skeleton2.exe

echo
echo "skeleton 2- simple2d"
echo
srun ./skeleton2-simple2d.exe

echo
echo "skeleton 2-heated plate"
echo
srun ./skeleton2-heated-plate.exe

echo
echo "skeleton 3"
echo
srun ./skeleton3.exe

echo
echo "skeleton 4"
echo
srun ./skeleton4.exe

echo
echo "deadlock"
echo
srun ./deadlock.exe
